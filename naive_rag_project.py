# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XfLwpvVIuCgdgs5P8hpyFN2YnnnTBOou
"""

!pip install datasets
!pip install llama_index

# 관련 패키지 임포트
import os
from datasets import load_dataset
from llama_index.core import Document
from llama_index.core import VectorStoreIndex
from llama_index.core.settings import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
import nest_asyncio
import pandas as pd
from docx import Document
import re

nest_asyncio.apply()

# 활용 LLM API 정보 설정
os.environ["OPENAI_API_KEY"] = ''

# 구글 코랩에서 필요한 라이브러리 설치
!pip install python-docx pandas

# 구글 코랩에서 파일 업로드
from google.colab import files
uploaded = files.upload()

# 업로드된 파일 이름 확인
for filename in uploaded.keys():
    file_path = '/content/' + filename
    print(f'File uploaded: {file_path}')

# DOCX 파일 읽기
doc = Document(file_path)

# 문서의 모든 텍스트 추출
full_text = []
for para in doc.paragraphs:
    full_text.append(para.text)
text = '\n'.join(full_text)

# 텍스트 분리
date_pattern = r"여행 날짜: (\d{4}년 \d{1,2}월 \d{1,2}일)"
location_pattern = r"(\d+\..+?)(?=\d+\.|여행 날짜:|$)"

dates = re.findall(date_pattern, text)
locations = re.split(date_pattern, text)[1:]

data = []
for i in range(0, len(locations), 2):
    date = locations[i].strip()
    location_texts = re.findall(location_pattern, locations[i+1], re.DOTALL)
    for loc in location_texts:
        loc_parts = loc.split('\n', 1)
        place = loc_parts[0].strip()
        content = loc_parts[1].strip() if len(loc_parts) > 1 else ''
        data.append([date, place, content])

# pandas 데이터프레임으로 변환
df = pd.DataFrame(data, columns=['Date', 'Place', 'Content'])

# 데이터프레임 출력
print(df.head())

# 데이터프레임을 CSV 파일로 저장
df.to_csv('/content/separated_travel_data.csv', index=False, encoding='utf-8')

# CSV 파일 다운로드
files.download('/content/separated_travel_data.csv')

# CSV 파일 로드
df = pd.read_csv('/content/separated_travel_data.csv')

# 데이터프레임을 라마인덱스 다큐먼트 객체로 변환
docs = []

for i, row in df.iterrows():
    docs.append(Document(
        text=row['Content'],
        extra_info={'date': row['Date'], 'place': row['Place']}
    ))

# RAG 파이프라인 글로벌 설정
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-small"
)

Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0)

# 벡터스토어 인덱스 설정
vector_index = VectorStoreIndex.from_documents(
    docs,
    use_asnyc=True
)

# 쿼리 엔진 설정
vector_query_engine = vector_index.as_query_engine(similarity_top_k=3)

# 질문을 통해 검색
question = '그때 갔던 그 카페 이름이 뭐지?'
response = vector_query_engine.query(question)

# 검색에 대한 답변 출력
print("답변:", response.response)

# 소스 노드 출력
for node in response.source_nodes:
    print(f"점수: {node.score}, 텍스트: {node.node.text}")

